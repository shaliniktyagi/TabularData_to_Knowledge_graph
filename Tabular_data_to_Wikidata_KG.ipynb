{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code is used for getting entities for CEA task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# ENTITY look up code, referenced from GitHub(2020) ernestojimenezruiz/tabular-data-semantics-py.\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class KG(Enum):\n",
    "        DBpedia = 0\n",
    "        Wikidata = 1\n",
    "        Google = 2        \n",
    "        All = 3\n",
    "class URI_KG(object):\n",
    "    dbpedia_uri_resource = 'http://dbpedia.org/resource/'\n",
    "    dbpedia_uri_property = 'http://dbpedia.org/property/'\n",
    "    \n",
    "    dbpedia_uri = 'http://dbpedia.org/ontology/'\n",
    "    wikidata_uri ='http://www.wikidata.org/'\n",
    "    schema_uri = 'http://schema.org/' \n",
    "\n",
    "    uris = list()\n",
    "    uris.append(dbpedia_uri)\n",
    "    uris.append(wikidata_uri)\n",
    "    uris.append(schema_uri)\n",
    "    \n",
    "    uris_resource = list()\n",
    "    uris_resource.append(dbpedia_uri_resource)\n",
    "    uris_resource.append(wikidata_uri)\n",
    "          \n",
    "    avoid_predicates=set()\n",
    "    avoid_predicates.add(\"http://dbpedia.org/ontology/wikiPageDisambiguates\")\n",
    "    avoid_predicates.add(\"http://dbpedia.org/ontology/wikiPageRedirects\")\n",
    "    avoid_predicates.add(\"http://dbpedia.org/ontology/wikiPageWikiLink\")\n",
    "    avoid_predicates.add(\"http://dbpedia.org/ontology/wikiPageID\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        ''''\n",
    "        '''\n",
    "class KGEntity(object):\n",
    "    \n",
    "    def __init__(self, enity_id, label, description, types, source):\n",
    "        \n",
    "        self.ident = enity_id\n",
    "        self.label = label\n",
    "        self.desc = description #sometimes provides a very concrete type or additional semantics\n",
    "        self.types = types  #set of semantic types\n",
    "        self.source = source  #KG of origin such as dbpedia, wikidata or google KG\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"<id: %s , label: %s, description: %s, types: %s, source: %s>\" % (self.ident, self.label,self.desc, self.types, self.source)\n",
    "    def __str__(self):\n",
    "        return \"<id: %s , label: %s, description: %s, types: %s, source: %s>\" % (self.ident, self.label, self.desc, self.types, self.source)\n",
    "    def getId(self):\n",
    "        return self.ident\n",
    "    \n",
    "    '''\n",
    "    One can retrieve all types or filter by KG: DBpedia, Wikidata and Google (Schema.org)\n",
    "    '''\n",
    "    def getTypes(self, kgfilter=KG.All):\n",
    "        if kgfilter==KG.All:\n",
    "            return self.types\n",
    "        else:\n",
    "            kg_uri = URI_KG.uris[kgfilter.value]\n",
    "            filtered_types = set()\n",
    "            for t in self.types:\n",
    "                if t.startswith(kg_uri):\n",
    "                    filtered_types.add(t)\n",
    "            \n",
    "            return filtered_types \n",
    "    \n",
    "    def getLabel(self):\n",
    "        return self.label\n",
    "    def getDescription(self):\n",
    "        return self.desc\n",
    "    def getSource(self):\n",
    "        return self.sourcec\n",
    "    def addType(self, cls):\n",
    "        self.types.add(cls)\n",
    "    def addTypes(self, types):\n",
    "        self.types.update(types)\n",
    "if __name__ == '__main__':\n",
    "    print(URI_KG.uris[KG.Wikidata.value])\n",
    "    print(KG.DBpedia.value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIKIDATA LOOKUP\n",
    "\n",
    "# Import the libraries \n",
    "import json\n",
    "from pprint import pprint\n",
    "import time\n",
    "from urllib import parse, request\n",
    "\n",
    "class KGLookup(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "    def __init__(self, lookup_url):\n",
    "        self.service_url = lookup_url\n",
    "    def getJSONRequest(self, params, attempts=3):\n",
    "        \n",
    "        try:\n",
    "            #urllib has been split up in Python 3. \n",
    "            #The urllib.urlencode() function is now urllib.parse.urlencode(), \n",
    "            #and the urllib.urlopen() function is now urllib.request.urlopen().\n",
    "            #url = service_url + '?' + urllib.urlencode(params)\n",
    "            url = self.service_url + '?' + parse.urlencode(params)\n",
    "            #print(url)\n",
    "            #response = json.loads(urllib.urlopen(url).read())\n",
    "\n",
    "            req = request.Request(url)\n",
    "            \n",
    "            req.add_header('Accept', 'application/json')\n",
    "            \n",
    "            response = json.loads(request.urlopen(req).read())\n",
    "            \n",
    "            return response\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Lookup '%s' failed. Attempts: %s\" % (url, str(attempts)))\n",
    "            time.sleep(60) #to avoid limit of calls, sleep 60s\n",
    "            attempts-=1\n",
    "            if attempts>0:\n",
    "                return self.getJSONRequest(params, attempts)\n",
    "            else:\n",
    "                return None\n",
    "'''\n",
    "Wikidata web search API\n",
    "'''\n",
    "class WikidataAPI(KGLookup):\n",
    "    '''\n",
    "    classdocs\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        super().__init__(self.getURL())\n",
    "\n",
    "    def getURL(self):\n",
    "        return \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "    def __createParams(self, query, limit, type='item'):\n",
    "        \n",
    "        params = {\n",
    "            'action': 'wbsearchentities',\n",
    "            'format' : 'json',\n",
    "            'search': query,\n",
    "            'type': type,\n",
    "            'limit': limit,\n",
    "            'language' : 'en'\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def getKGName(self):\n",
    "        return 'Wikidata'\n",
    "    \n",
    "    '''\n",
    "    Returns list of ordered entities according to relevance: wikidata\n",
    "    '''\n",
    "    def __extractKGEntities(self, json, filter=''):\n",
    "        \n",
    "        entities = list()\n",
    "        \n",
    "        for element in json['search']:\n",
    "            #empty list of type from wikidata lookup\n",
    "            types = set()\n",
    "\n",
    "            description=''\n",
    "            if 'description' in element:\n",
    "                description = element['description']\n",
    "            kg_entity = KGEntity(\n",
    "                element['concepturi'],\n",
    "                element['label'],\n",
    "                description,\n",
    "                types,\n",
    "                self.getKGName()\n",
    "                )\n",
    "            \n",
    "            #We filter according to givem URI\n",
    "            if filter=='' or element['concepturi']==filter:\n",
    "                entities.append(kg_entity)\n",
    "        #for entity in entities:\n",
    "        #    print(entity)    \n",
    "        return entities\n",
    "\n",
    "    def getKGEntities(self, query, limit, type='item', filter=''):        \n",
    "        json = self.getJSONRequest(self.__createParams(query, limit, type), 3)     \n",
    "        \n",
    "        if json==None:\n",
    "            print(\"None results for\", query)\n",
    "            return list()\n",
    "        return self.__extractKGEntities(json, filter) #Optionally filter by URI\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table_id</th>\n",
       "      <th>Row_id</th>\n",
       "      <th>Column_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>88TAWLJF</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Table_id  Row_id  Column_id\n",
       "0  88TAWLJF       1          0\n",
       "1  88TAWLJF       1          2\n",
       "2  88TAWLJF       2          0\n",
       "3  88TAWLJF       2          2\n",
       "4  88TAWLJF       3          0\n",
       "5  88TAWLJF       3          2\n",
       "6  88TAWLJF       4          0\n",
       "7  88TAWLJF       4          2\n",
       "8  88TAWLJF       5          0\n",
       "9  88TAWLJF       5          2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "os.getcwd()\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\01-18-20\\\\Documents\\\\City_University\\\\round3\\\\CEA_Round3_Targets.csv\", header=None, nrows=10) # CEA targets file is used to read\n",
    "df.columns=[\"Table_id\", \"Row_id\", \"Column_id\"] # Assign the header to the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cell\"] = \"\" # Adding new column in the dataframe df to fetch the cell value in it\n",
    "\n",
    "#df.head()\n",
    "\n",
    "def capitalize_word(word):\n",
    "    return word.capitalize()\n",
    "\n",
    "def remove_special_signs(word):\n",
    "    result = \"\"\n",
    "    for w in word:\n",
    "        if w.isalpha() or w==\"'\" or w.isspace() or w== \"©\" or w == \"Ã\":\n",
    "            result += w \n",
    "    return result\n",
    "\n",
    "def replace_space(word):\n",
    "    try:\n",
    "        return word.replace(\" \", \"_\")\n",
    "    except:\n",
    "        print(word)\n",
    "        return word\n",
    "    \n",
    "def get_entity(df,  row_id, column_id): \n",
    "    try:\n",
    "        cell = df.iloc[row_id, column_id]\n",
    "        #cell = remove_special_signs(cell)\n",
    "        cell = capitalize_word(cell)\n",
    "        cell = replace_space(cell)\n",
    "        return cell\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "df = df.sort_values(\"Table_id\").reset_index(drop=True)\n",
    "first_table = df[\"Table_id\"].unique()[0]\n",
    "#first_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_for_row(row):\n",
    "    global first_table\n",
    "    table_id = row[\"Table_id\"]\n",
    "    if first_table == table_id:\n",
    "         df = pd.read_csv(f\"C:\\\\Users\\\\01-18-20\\\\Documents\\\\City_University\\\\round3\\\\Tables_Round3\\\\tables\\\\{table_id}.csv\", header=None)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"C:\\\\Users\\\\01-18-20\\\\Documents\\\\City_University\\\\round3\\\\Tables_Round3\\\\tables\\\\{table_id}.csv\", header=None)\n",
    "        first_table = table_id\n",
    "    df.head()\n",
    "    \n",
    "    row_id = row[\"Row_id\"]\n",
    "    column_id = row[\"Column_id\"]\n",
    "    cell = get_entity(df, row_id, column_id,)\n",
    "    row[\"Cell\"]=cell\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(function_for_row, axis=1)\n",
    "df[\"Wikidata_Entity\"]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\01-18-20\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<id: http://www.wikidata.org/entity/Q11886086 , label: Oulu YMCA, description: Finnish society, types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q13567944 , label: Koskikeskus, description: city district in Oulu, Finland, types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q10495580 , label: Finnish Art Society, description: Finnish society, types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q1792414 , label: Kunsthalle Helsinki, description: art exhibition venue in Helsinki, Finland, types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q4208381 , label: BMS World Mission, description: Christian missionary society, types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q1009242 , label: Didcot, description: town and civil parish in Oxfordshire, UK; formerly in Berkshire, types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q1139380 , label: National Speleological Society, description: society, types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q79860 , label: Huntsville, description: county seat of Madison County, Alabama, United States, types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q3232649 , label: Les Films du Carrosse, description: , types: set(), source: Wikidata>]\n",
      "[<id: http://www.wikidata.org/entity/Q90 , label: Paris, description: capital and largest city of France, types: set(), source: Wikidata>]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the cell entity for the cell value from KG and store in to seperate column\n",
    "for i in range (0, 10):\n",
    "    if __name__ == '__main__':\n",
    "        query = df.loc[i][\"Cell\"]\n",
    "        limit=1 # Limit variable is varied  up to 10\n",
    "        type=\"item\" \n",
    "\n",
    "        wikidata = WikidataAPI()\n",
    "        entities = wikidata.getKGEntities(query, limit, type)\n",
    "        df[\"Wikidata_Entity\"][i] = entities\n",
    "        print(entities)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is for save the file and download from Google Colab platfrom\n",
    "#from google.colab import files\n",
    "#df.to_csv('CEA_Round3_result_37.csv') \n",
    "#files.download('CEA_Round3_result_37.csv') # This csv file is the result for CEA task in the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to check the duplicate values in the outcome file \n",
    "\n",
    "df = pd.read_csv(\"combine_output_final - Copy.csv\")\n",
    "\n",
    "duplicateDFRow = df[df.duplicated()]\n",
    "print(duplicateDFRow)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "print(df)\n",
    "\n",
    "#df.to_csv('CEA_result_task1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section of code is for calculating cosine similarity in string matching for CEA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file is taken after getting entities from Wikidata KG \n",
    "df = pd.read_csv(\"C:\\\\Users\\\\01-18-20\\\\Documents\\\\City_University\\\\Round2\\\\filnal_process_data\\\\file_06.csv\", nrows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text in to lower case in some columns from the dataframe\n",
    "df[\"Cell\"] = df[\"Cell\"].str.lower()\n",
    "df[\"Label1\"] = df[\"Label1\"].str.lower()\n",
    "df[\"Label2\"] = df[\"Label2\"].str.lower()\n",
    "df[\"Label3\"] = df[\"Label3\"].str.lower()\n",
    "df[\"Label4\"] = df[\"Label4\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the _ from the Cell column to match the string\n",
    "pec_chars = [\"_\"]\n",
    "for char in spec_chars:\n",
    "    df['Cell'] = df['Cell'].str.replace(char, ' ')\n",
    "#df.fillna('empty cell', inplace=True)\n",
    "df1=df\n",
    "df1['Label1']= df1['Label1'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['simi']=''\n",
    "df1['Similar_label']=''\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "for k in range(0, 1):\n",
    "    #print(row['c1'], row['c2'])\n",
    "     \n",
    "    test=[]\n",
    "\n",
    "# X = input(\"Enter first string: \").lower() \n",
    "# Y = input(\"Enter second string: \").lower() \n",
    "    X = df1.iloc[k]['Cell']\n",
    "    Y1 =df1.iloc[k]['Label1']\n",
    "    Y2 =df1.iloc[k]['Label2']\n",
    "    Y3 =df1.iloc[k]['Label3']\n",
    "    Y4 =df1.iloc[k]['Label4']\n",
    "# tokenization \n",
    "\n",
    "    X_list = word_tokenize(X)  \n",
    "    Y1_list = word_tokenize(Y1) \n",
    "    Y2_list = word_tokenize(Y2) \n",
    "    Y3_list = word_tokenize(Y3) \n",
    "    Y4_list = word_tokenize(Y4) \n",
    "  \n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[]\n",
    "    l2 =[] \n",
    "  \n",
    "    # remove stop words from the string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y1_set = {w for w in Y1_list if not w in sw} \n",
    "    Y2_set = {w for w in Y2_list if not w in sw}\n",
    "    Y3_set = {w for w in Y3_list if not w in sw}\n",
    "    Y4_set = {w for w in Y4_list if not w in sw}\n",
    "\n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector1 = X_set.union(Y1_set)  \n",
    "    for w in rvector1: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y1_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "  \n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector1)): \n",
    "            c+= l1[i]*l2[i]\n",
    "    try:       \n",
    "        cosine1 = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        cosine1 = 0\n",
    "    test.append(cosine1)\n",
    "    l1 =[]\n",
    "    l2 =[] \n",
    "\n",
    "    rvector2 = X_set.union(Y2_set)  \n",
    "    for w in rvector2: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y2_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "  \n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector2)): \n",
    "            c+= l1[i]*l2[i]\n",
    "    try:\n",
    "        cosine2 = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        cosine2 = 0\n",
    "   \n",
    "    #print(\"similarity: \", cosine2) \n",
    "    test.append(cosine2)\n",
    "\n",
    "    l1 =[]\n",
    "    l2 =[] \n",
    "\n",
    "    rvector3 = X_set.union(Y3_set)  \n",
    "    for w in rvector3: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y3_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "  \n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector3)): \n",
    "            c+= l1[i]*l2[i]\n",
    "    try:\n",
    "        cosine3 = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    except ZeroDivisionError:\n",
    "        cosine3 = 0\n",
    "    ##print(\"similarity: \", cosine3) \n",
    "    test.append(cosine3)\n",
    "\n",
    "    l1 =[]\n",
    "    l2 =[] \n",
    "\n",
    "    rvector4 = X_set.union(Y4_set)  \n",
    "    for w in rvector4: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y4_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "  \n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector4)): \n",
    "            c+= l1[i]*l2[i]\n",
    "    try:\n",
    "        cosine4 = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    except ZeroDivisionError:\n",
    "        cosine4 = 0\n",
    "    test.append(cosine4)\n",
    "    similarity_max = max(float(sub) for sub in test)\n",
    "\n",
    "    maxpos = test.index(max(test)) \n",
    "    if maxpos == 0:\n",
    "        df1[\"Similar_label\"][k] = df1[\"Wikidata_Entity1\"][k]\n",
    "    elif maxpos == 1:\n",
    "        df1[\"Similar_label\"][k] = df1[\"Wikidata_Entity2\"][k]\n",
    "    elif maxpos == 2:\n",
    "        df1[\"Similar_label\"][k] = df1[\"Wikidata_Entity3\"][k]\n",
    "    else:\n",
    "        df1[\"Similar_label\"][k] = df1[\"Wikidata_Entity4\"][k]\n",
    "    \n",
    "    #df1[\"simi\"][k] = test\n",
    "        #print(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('simi1.csv') # save the dataframe to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code section is used for getting the entity for missing values. this code is used the DBpedia KG lookup and the DBpedia endpoint to get the same entity in Wikidata KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities:  100\n",
      "http://dbpedia.org/resource/James_Dunwoody_Bulloch\n",
      "http://dbpedia.org/resource/Cambodia\n",
      "http://dbpedia.org/resource/Dapeng_(state)\n",
      "http://dbpedia.org/resource/Recherla_Nayaks\n",
      "http://dbpedia.org/resource/Archaic–Early_Basketmaker_Era\n",
      "http://dbpedia.org/resource/United_Kingdom_of_Portugal,_Brazil_and_the_Algarves\n",
      "http://dbpedia.org/resource/Stem_duchy\n",
      "http://dbpedia.org/resource/Batavian_Republic\n",
      "http://dbpedia.org/resource/Northern_Han\n",
      "http://dbpedia.org/resource/Fiscal_year\n",
      "http://dbpedia.org/resource/Saloum\n",
      "http://dbpedia.org/resource/Taifa_of_Arjona\n",
      "http://dbpedia.org/resource/Coins_of_the_Italian_lira\n",
      "http://dbpedia.org/resource/Los_Angeles_in_the_1920s\n",
      "http://dbpedia.org/resource/German_Federal_Republic\n",
      "http://dbpedia.org/resource/Religion_in_Western_Ganga_kingdom\n",
      "http://dbpedia.org/resource/French_West_Africa\n",
      "http://dbpedia.org/resource/Tusculan_Papacy\n",
      "http://dbpedia.org/resource/Reign_of_Terror\n",
      "http://dbpedia.org/resource/Kamboja_(name)\n",
      "http://dbpedia.org/resource/RCCA_Security\n",
      "http://dbpedia.org/resource/Mendorra\n",
      "http://dbpedia.org/resource/Luwu\n",
      "http://dbpedia.org/resource/Belarus\n",
      "http://dbpedia.org/resource/Valluvanad_(southern_Malabar)\n",
      "http://dbpedia.org/resource/Banten_Sultanate\n",
      "http://dbpedia.org/resource/Standard_trading_conditions\n",
      "http://dbpedia.org/resource/Mankessim_Kingdom\n",
      "http://dbpedia.org/resource/Great_Moravia\n",
      "http://dbpedia.org/resource/Strandzha_Commune\n",
      "http://dbpedia.org/resource/Gambia_Colony_and_Protectorate\n",
      "http://dbpedia.org/resource/Kingdom_of_France_(1791–92)\n",
      "http://dbpedia.org/resource/Paracanonical_texts_(Theravada_Buddhism)\n",
      "http://dbpedia.org/resource/Toro_Kingdom\n",
      "http://dbpedia.org/resource/Porirua_(New_Zealand_electorate)\n",
      "http://dbpedia.org/resource/Hadiya_Sultanate\n",
      "http://dbpedia.org/resource/Council_of_the_District_of_Columbia_Period_13\n",
      "http://dbpedia.org/resource/Oceania_Football_Confederation\n",
      "http://dbpedia.org/resource/Federation_of_Nigeria\n",
      "http://dbpedia.org/resource/Hamidids\n",
      "http://dbpedia.org/resource/Ahiman_Rezon\n",
      "http://dbpedia.org/resource/Kings_of_Uí_Maine\n",
      "http://dbpedia.org/resource/Walkenried_Abbey\n",
      "http://dbpedia.org/resource/Kingdom_of_Jolof\n",
      "http://dbpedia.org/resource/Pomeroon_(colony)\n",
      "http://dbpedia.org/resource/Istanu\n",
      "http://dbpedia.org/resource/Ngalangi\n",
      "http://dbpedia.org/resource/Dodging_and_burning\n",
      "http://dbpedia.org/resource/County_of_Toulouse\n",
      "http://dbpedia.org/resource/Law_of_Serbia\n",
      "http://dbpedia.org/resource/Lithuania\n",
      "http://dbpedia.org/resource/History_of_Poland_during_the_Jagiellonian_dynasty\n",
      "http://dbpedia.org/resource/Dutch_Empire\n",
      "http://dbpedia.org/resource/Grand_Duchy_of_Berg\n",
      "http://dbpedia.org/resource/Economy_of_the_Hoysala_Empire\n",
      "http://dbpedia.org/resource/Netherlands-Indonesian_Union\n",
      "http://dbpedia.org/resource/Fief_of_Purmerend,_Purmerland_and_Ilpendam\n",
      "http://dbpedia.org/resource/Khorshidi_dynasty\n",
      "http://dbpedia.org/resource/Schöntal_Abbey\n",
      "http://dbpedia.org/resource/List_of_ambassadors_of_the_United_Kingdom_to_Haiti\n",
      "http://dbpedia.org/resource/List_of_ambassadors_of_the_United_Kingdom_to_Portugal\n",
      "http://dbpedia.org/resource/Portuguese_Guinea\n",
      "http://dbpedia.org/resource/Sport_for_Food\n",
      "http://dbpedia.org/resource/Magadha_Kingdom\n",
      "http://dbpedia.org/resource/Maternity_leave_in_the_United_States\n",
      "http://dbpedia.org/resource/Karluks\n",
      "http://dbpedia.org/resource/Interregnum_of_World_Chess_Champions\n",
      "http://dbpedia.org/resource/Maratha_Empire\n",
      "http://dbpedia.org/resource/Kalingga_Kingdom\n",
      "http://dbpedia.org/resource/Second_Occupation_of_Cuba\n",
      "http://dbpedia.org/resource/Corrections\n",
      "http://dbpedia.org/resource/Embassy_of_the_United_Kingdom,_Tehran\n",
      "http://dbpedia.org/resource/Waldburg-Sonnenburg\n",
      "http://dbpedia.org/resource/Halaf_culture\n",
      "http://dbpedia.org/resource/Ascension_Island\n",
      "http://dbpedia.org/resource/Neo-Celtic_Christianity\n",
      "http://dbpedia.org/resource/History_of_the_Philippines_(1946–65)\n",
      "http://dbpedia.org/resource/Frankfurt_Constitution\n",
      "http://dbpedia.org/resource/Ficquelmont_family\n",
      "http://dbpedia.org/resource/Aerican_Empire\n",
      "http://dbpedia.org/resource/Classical_Hollywood_cinema\n",
      "http://dbpedia.org/resource/Count_of_Malta\n",
      "http://dbpedia.org/resource/Juliana_Republic\n",
      "http://dbpedia.org/resource/Principality_of_Freedonia\n",
      "http://dbpedia.org/resource/Oscar_season\n",
      "http://dbpedia.org/resource/Pueblo_I_Period\n",
      "http://dbpedia.org/resource/Electoral_district_of_City_of_Sydney\n",
      "http://dbpedia.org/resource/Provisional_Administration_of_South_Ossetia\n",
      "http://dbpedia.org/resource/Slump_(sports)\n",
      "http://dbpedia.org/resource/Kingdom_of_Dagbon\n",
      "http://dbpedia.org/resource/Hard_Spell\n",
      "http://dbpedia.org/resource/Connecticut_Colony\n",
      "http://dbpedia.org/resource/Lanfang_Republic\n",
      "http://dbpedia.org/resource/Árpád_dynasty\n",
      "http://dbpedia.org/resource/Maku_Khanate\n",
      "http://dbpedia.org/resource/Special_administrative_regions_of_China\n",
      "http://dbpedia.org/resource/Czech_and_Slovak_Federal_Republic\n",
      "http://dbpedia.org/resource/O'Kennedy\n",
      "http://dbpedia.org/resource/Billingsgate_Fish_Market\n",
      "http://dbpedia.org/resource/Brunei\n",
      "Time: 0.22696232795715332\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\01-18-20\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# DBpedia ENDPOINT refernced from GitHub(2020) ernestojimenezruiz/tabular-data-semantics-py\n",
    "import time\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import sys\n",
    "\n",
    "\n",
    "class SPARQLEndpoint(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "    def __init__(self, endpoint_url):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        #\"http://dbpedia.org/sparql\"\n",
    "        self.sparqlw = SPARQLWrapper(endpoint_url)\n",
    "        \n",
    "        self.sparqlw.setReturnFormat(JSON)\n",
    "        \n",
    "        \n",
    "    def getSameEntities(self, ent):\n",
    "        \n",
    "        query = self.createSPARQLQuerySameAsEntities(ent)\n",
    "        \n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getEnglishLabelsForEntity(self, ent):\n",
    "        \n",
    "        query = self.createEnglishLabelsForURI(ent)\n",
    "        \n",
    "        return self.getQueryResultsArityOne4Literals(query)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def getEntitiesForType(self, cls, offset=0, limit=1000):\n",
    "        \n",
    "        query = self.createSPARQLEntitiesForClass(cls, offset, limit)\n",
    "        \n",
    "        #print(query)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "    \n",
    "    \n",
    "    def getEntitiesLabelsForType(self, cls, offset=0, limit=1000):\n",
    "        \n",
    "        query = self.createSPARQLEntitiesLabelsForClass(cls, offset, limit)\n",
    "        \n",
    "        #print(query)\n",
    "        \n",
    "        #Second element is a string so we do not filter it\n",
    "        return self.getQueryResultsArityTwo(query, True, False)\n",
    "    \n",
    "        \n",
    "    def getTypesForEntity(self, entity):\n",
    "        \n",
    "        query = self.createSPARQLQueryTypesForSubject(entity)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "    \n",
    "\n",
    "    def getAllTypesForEntity(self, entity):\n",
    "        \n",
    "        query = self.createSPARQLQueryAllTypesForSubject(entity)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "        \n",
    "\n",
    "    def getEquivalentClasses(self, uri_class):\n",
    "        \n",
    "        query = self.createSPARQLQueryEquivalentClasses(uri_class)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "    \n",
    "    def getAllSuperClasses(self, uri_class):\n",
    "        \n",
    "        query = self.createSPARQLQueryAllSuperClassesFoClass(uri_class)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "    \n",
    "    \n",
    "    def getDistanceToAllSuperClasses(self, uri_class):\n",
    "        \n",
    "        query = self.createSPARQLQueryDistanceToAllSuperClassesForClass(uri_class)\n",
    "             \n",
    "        super2dist = self.getQueryResultsArityTwo(query, False, False)\n",
    "        \n",
    "        #Filter top classes   \n",
    "        for top_cls in URI_KG.avoid_top_concepts:\n",
    "            super2dist.pop(top_cls, None)\n",
    "    \n",
    "    \n",
    "        return super2dist\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getAllSubClasses(self, uri_class):\n",
    "        \n",
    "        query = self.createSPARQLQueryAllSubClassesFoClass(uri_class)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "    \n",
    "    \n",
    "    def getDistanceToAllSubClasses(self, uri_class, max_level=-1):\n",
    "        \n",
    "        query = self.createSPARQLQueryDistanceToAllSubClassesForClass(uri_class)\n",
    "             \n",
    "        sub2dist = self.getQueryResultsArityTwo(query, False, False)\n",
    "        \n",
    "        if max_level>0:\n",
    "            sub2dist_new = sub2dist.copy()\n",
    "            for scls in sub2dist.keys():\n",
    "                #Only one element in set\n",
    "                if int(sorted(sub2dist_new[scls])[0])>max_level:\n",
    "                    sub2dist_new.pop(scls)\n",
    "                \n",
    "            return sub2dist_new\n",
    "        else:\n",
    "            return sub2dist\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    def getPredicatesForSubject(self, subject_entity, limit=1000):\n",
    "        \n",
    "        query = self.createSPARQLQueryPredicatesForSubject(subject_entity, limit)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "        \n",
    "    def getPredicatesForObject(self, obj_entity, limit=1000):\n",
    "        \n",
    "        query = self.createSPARQLQueryPredicatesForObject(obj_entity, limit)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Exploits the domain types of the properties\n",
    "    def getTypesUsingPredicatesForSubject(self, subject_entity, limit=1000):\n",
    "        \n",
    "        query = self.createSPARQLQueryDomainTypesOfPredicatesForSubject(subject_entity, limit)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "        \n",
    "    #Exploits the range types of the properties\n",
    "    def getTypesUsingPredicatesForObject(self, obj_entity, limit=1000):\n",
    "        \n",
    "        query = self.createSPARQLQueryRangeTypesOfPredicatesForObject(obj_entity, limit)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    def getTopTypesUsingPredicatesForSubject(self, subject_entity, limit=5):\n",
    "        \n",
    "        query = self.createSPARQLQueryDomainTypesCountOfPredicatesForSubject(subject_entity, limit)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "        \n",
    "    #Exploits the range types of the properties\n",
    "    def getTopTypesUsingPredicatesForObject(self, obj_entity, limit=5):\n",
    "        \n",
    "        query = self.createSPARQLQueryRangeTypesCountOfPredicatesForObject(obj_entity, limit)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    def getTriplesForSubject(self, subject_entity, limit=1000):\n",
    "        \n",
    "        query = self.createSPARQLQueryTriplesForSubject(subject_entity, limit)\n",
    "        \n",
    "        #print(query)        \n",
    "        #print(self.getQueryResultsArityTwo(query, False, False))\n",
    "        \n",
    "        return self.getQueryResultsArityTwo(query, False, False)\n",
    "        \n",
    "        \n",
    "    def getTriplesForObject(self, obj_entity, limit=1000):\n",
    "        \n",
    "        query = self.createSPARQLQueryTriplesForObject(obj_entity, limit)\n",
    "        \n",
    "        return self.getQueryResultsArityTwo(query)    \n",
    "    \n",
    "    \n",
    "    def getSomeValuesForPredicate(self, predicate, limit=100):\n",
    "        \n",
    "        query = self.createSPARQLQuerySomeValuesForPredicate(predicate, limit)\n",
    "        \n",
    "        return self.getQueryResultsArityOne(query)    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def getQueryResults(self, query, attempts=5):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            self.sparqlw.setQuery(query)\n",
    "            \n",
    "            return self.sparqlw.query().convert()\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Query '%s' failed. Attempts: %s\" % (query, str(attempts)))\n",
    "            time.sleep(60) #to avoid limit of calls, sleep 60s\n",
    "            attempts-=1\n",
    "            if attempts>0:\n",
    "                return self.getQueryResults(query, attempts)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "    def getQueryResultsArityOne(self, query, filter_uri=True):\n",
    "        \n",
    "        \n",
    "        results = self.getQueryResults(query, 3)\n",
    "            \n",
    "            \n",
    "        result_set = set()\n",
    "    \n",
    "        if results==None:\n",
    "            print(\"None results for\", query)\n",
    "            return result_set\n",
    "            \n",
    "    \n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            #print(result)\n",
    "            #print(result[\"uri\"][\"value\"])\n",
    "            uri_value = result[\"uri\"][\"value\"]\n",
    "            \n",
    "            if not filter_uri or uri_value.startswith(URI_KG.dbpedia_uri) or uri_value.startswith(URI_KG.wikidata_uri) or uri_value.startswith(URI_KG.schema_uri) or uri_value.startswith(URI_KG.dbpedia_uri_resource) or uri_value.startswith(URI_KG.dbpedia_uri_property): \n",
    "                result_set.add(uri_value)\n",
    "        \n",
    "        \n",
    "        return result_set\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getQueryResultsArityOne4Literals(self, query):\n",
    "        \n",
    "        \n",
    "        results = self.getQueryResults(query, 3)\n",
    "            \n",
    "            \n",
    "        result_set = set()\n",
    "    \n",
    "        if results==None:\n",
    "            print(\"None results for\", query)\n",
    "            return result_set\n",
    "            \n",
    "    \n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            \n",
    "            value = result[\"literal\"][\"value\"]\n",
    "            \n",
    "            result_set.add(value)\n",
    "        \n",
    "        \n",
    "        return result_set\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def getQueryResultsArityTwo(self, query, filter_outA=True, filter_outB=True):\n",
    "        \n",
    "        #self.sparqlw.setQuery(query)\n",
    "        #results = self.sparqlw.query().convert()\n",
    "\n",
    "        results = self.getQueryResults(query, 3)\n",
    "    \n",
    "        result_dict = dict()\n",
    "        \n",
    "        if results==None:\n",
    "            print(\"None results for\", query)\n",
    "            return result_dict\n",
    "        \n",
    "    \n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            #print(result)\n",
    "            #print(result[\"uri\"][\"value\"])\n",
    "            outA_value = result[\"outA\"][\"value\"]\n",
    "            outB_value = result[\"outB\"][\"value\"]\n",
    "            \n",
    "            \n",
    "            if not filter_outA or outA_value.startswith(URI_KG.dbpedia_uri) or outA_value.startswith(URI_KG.wikidata_uri) or outA_value.startswith(URI_KG.schema_uri) or outA_value.startswith(URI_KG.dbpedia_uri_resource) or outA_value.startswith(URI_KG.dbpedia_uri_property): \n",
    "                \n",
    "                    if not filter_outB or outB_value.startswith(URI_KG.dbpedia_uri) or outB_value.startswith(URI_KG.wikidata_uri) or outB_value.startswith(URI_KG.schema_uri) or outB_value.startswith(URI_KG.dbpedia_uri_resource) or outB_value.startswith(URI_KG.dbpedia_uri_property):\n",
    "                        \n",
    "                        if outA_value not in result_dict:\n",
    "                            result_dict[outA_value] = set() \n",
    "                        \n",
    "                        result_dict[outA_value].add(outB_value)\n",
    "                \n",
    "        \n",
    "        return result_dict\n",
    "        \n",
    "    \n",
    "    def createSPARQLQueryTriplesForObject(self, obj, limit=1000):\n",
    "        \n",
    "        props_to_filter=\"\"\n",
    "        for p in URI_KG.avoid_predicates:\n",
    "            props_to_filter+=\"<\" + p + \">,\" \n",
    "        props_to_filter = props_to_filter[0:len(props_to_filter)-1]\n",
    "        \n",
    "        #props_to_filter = \",\".join(URI_KG.avoid_predicates)        \n",
    "        \n",
    "        return \"SELECT DISTINCT ?outA ?outB WHERE { ?outA ?outB <\" + obj + \"> . FILTER( ?outB NOT IN(\"+ props_to_filter+\")) } limit \" + str(limit)\n",
    "        \n",
    "    def createSPARQLQueryTriplesForSubject(self, subject, limit=1000):\n",
    "        \n",
    "        props_to_filter=\"\"\n",
    "        for p in URI_KG.avoid_predicates:\n",
    "            props_to_filter+=\"<\" + p + \">,\"\n",
    "            #props_to_filter+= p + \", \" \n",
    "        props_to_filter = props_to_filter[0:len(props_to_filter)-1]\n",
    "        \n",
    "        #props_to_filter = \",\".join(URI_KG.avoid_predicates)\n",
    "        \n",
    "        return \"SELECT DISTINCT ?outA ?outB WHERE { <\" + subject + \"> ?outA ?outB  FILTER( ?outA NOT IN(\"+ props_to_filter+\")) } limit \" + str(limit)\n",
    "        #return \"SELECT DISTINCT ?outA ?outB WHERE { <\" + subject + \"> ?outA ?outB . } limit \" + str(limit)\n",
    "    \n",
    "    \n",
    "    def createSPARQLQueryPredicatesForSubject(self, subject, limit=1000):\n",
    "        return \"SELECT DISTINCT ?uri WHERE { <\" + subject + \"> ?uri [] . } limit \" + str(limit)\n",
    "    \n",
    "    def createSPARQLQueryPredicatesForObject(self, obj, limit=1000):\n",
    "        return \"SELECT DISTINCT ?uri WHERE { [] ?uri <\" + obj + \"> . } limit \" + str(limit)\n",
    "    \n",
    "    \n",
    "    def createSPARQLQueryDomainTypesOfPredicatesForSubject(self, subject, limit=1000):\n",
    "        return \"SELECT DISTINCT ?uri WHERE { <\" + subject + \"> ?p [] . ?p rdfs:domain ?uri . } limit \" + str(limit)\n",
    "    \n",
    "    def createSPARQLQueryRangeTypesOfPredicatesForObject(self, obj, limit=1000):\n",
    "        return \"SELECT DISTINCT ?uri WHERE { [] ?p <\" + obj + \"> . ?p rdfs:range ?uri . } limit \" + str(limit)\n",
    "    \n",
    "    \n",
    "    def createSPARQLQuerySomeValuesForPredicate(self, predicate, limit=100):\n",
    "        return \"SELECT DISTINCT ?uri WHERE { ?s <\" + predicate + \"> ?uri . } limit \" + str(limit)\n",
    "    \n",
    "       \n",
    "    #SELECT DISTINCT ?outA COUNT(?outA) as ?outB WHERE { [] ?p <http://dbpedia.org/resource/Scotland> . ?p rdfs:range ?outA . } GROUP BY ?outA ORDER BY DESC(?outB) limit 3\n",
    "    #SELECT DISTINCT ?outA WHERE { [] ?p <http://dbpedia.org/resource/Scotland> . ?p rdfs:range ?outA . } GROUP BY ?outA ORDER BY DESC(COUNT(?outA)) limit 3\n",
    "    #SELECT DISTINCT ?outA WHERE { <http://dbpedia.org/resource/Allan_Pinkerton> ?p [] . ?p rdfs:domain ?outA . } GROUP BY ?outA ORDER BY DESC(COUNT(?outA)) limit 3\n",
    "    #SELECT DISTINCT ?outA COUNT(?outA) as ?outB WHERE { <http://dbpedia.org/resource/Allan_Pinkerton> ?p [] . ?p rdfs:domain ?outA . } GROUP BY ?outA ORDER BY DESC(?outB) limit 3 \n",
    "    \n",
    "    def createSPARQLQueryDomainTypesCountOfPredicatesForSubject(self, subject, limit=3):\n",
    "        return \"SELECT DISTINCT ?uri WHERE { <\" + subject + \"> ?p [] . ?p rdfs:domain ?uri . } GROUP BY ?uri HAVING (COUNT(?uri) > 3) ORDER BY DESC(COUNT(?uri)) limit \" + str(limit)\n",
    "    \n",
    "    def createSPARQLQueryRangeTypesCountOfPredicatesForObject(self, obj, limit=3):\n",
    "        return \"SELECT DISTINCT ?uri WHERE { [] ?p <\" + obj + \"> . ?p rdfs:range ?uri . } GROUP BY ?uri HAVING (COUNT(?uri) > 3) ORDER BY DESC(COUNT(?uri)) limit \" + str(limit)\n",
    "    \n",
    "    \n",
    "    def createEnglishLabelsForURI(self, uri_subject):\n",
    "        return \"SELECT DISTINCT ?literal WHERE { <\" + uri_subject + \"> rdfs:label ?literal . FILTER( langMatches(lang(?literal), 'en')) }\"\n",
    "    \n",
    "\n",
    "class DBpediaEndpoint(SPARQLEndpoint):\n",
    "    '''\n",
    "    classdocs\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        super().__init__(self.getEndpoint())\n",
    "        #\"http://dbpedia.org/sparql\"\n",
    "       \n",
    "        \n",
    "    def getEndpoint(self):\n",
    "        return \"http://dbpedia.org/sparql\"\n",
    "\n",
    "    def getWikiPageRedirect(self, uri_entity):\n",
    "        \n",
    "        query = self.createSPARQLQueryWikiPageRedirects(uri_entity)        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    " \n",
    "    def getWikiPageRedirectFrom(self, uri_entity):\n",
    "        \n",
    "        query = self.createSPARQLQueryWikiPageRedirectsFrom(uri_entity)        \n",
    "        return self.getQueryResultsArityOne(query)\n",
    "    \n",
    "    def createSPARQLEntitiesForClass(self, class_uri, offset=0, limit=1000):\n",
    "            \n",
    "        return \"SELECT DISTINCT ?uri WHERE { ?uri <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <\" + class_uri + \"> . } ORDER BY RAND() OFFSET \" + str(offset) + \" limit \" + str(limit)\n",
    "        #return \"SELECT DISTINCT ?uri WHERE { ?uri a dbo:Country . } ORDER BY RAND() limit \" + str(limit)\n",
    "    \n",
    "    def createSPARQLEntitiesLabelsForClass(self, class_uri, offset=0, limit=1000):\n",
    "            \n",
    "        return \"SELECT DISTINCT ?outA ?outB WHERE { ?outA <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <\" + class_uri + \"> . ?outA rdfs:label ?outB . FILTER( langMatches(lang(?outB), 'en')) } ORDER BY RAND() OFFSET \" + str(offset) + \" limit \" + str(limit)\n",
    "        #Lang restriction required\n",
    "        #return \"SELECT DISTINCT ?outA ?outB WHERE { ?outA <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <\" + class_uri + \"> . ?outA rdfs:label ?outB . } ORDER BY RAND() OFFSET \" + str(offset) + \" limit \" + str(limit)\n",
    "    #def createEnglishLabelsForURI(self, uri_subject):\n",
    "    #    return \"SELECT DISTINCT ?literal WHERE { <\" + uri_subject + \"> rdfs:label ?literal . FILTER( langMatches(lang(?literal), 'en')) }\"\n",
    "\n",
    "    def createSPARQLQueryTypesForSubject(self, uri_subject):\n",
    "            \n",
    "        return \"SELECT DISTINCT ?uri WHERE { <\" + uri_subject + \"> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?uri . }\"\n",
    "    \n",
    "    def createSPARQLQueryWikiPageRedirects(self, uri_subject):\n",
    "            \n",
    "        return \"SELECT DISTINCT ?uri WHERE { <\" + uri_subject + \"> <http://dbpedia.org/ontology/wikiPageRedirects> ?uri . }\"\n",
    "    \n",
    "    def createSPARQLQueryWikiPageRedirectsFrom(self, uri_object):\n",
    "            \n",
    "        return \"SELECT DISTINCT ?uri WHERE { ?uri <http://dbpedia.org/ontology/wikiPageRedirects> <\" + uri_object + \"> . }\"\n",
    "        \n",
    "    def createSPARQLQueryAllTypesForSubject(self, uri_subject):\n",
    "            \n",
    "        return \"SELECT DISTINCT ?uri \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"{<\" + uri_subject + \"> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?dt . \" \\\n",
    "        + \"?dt <http://www.w3.org/2000/01/rdf-schema#subClassOf>* ?uri \" \\\n",
    "        + \"}\" \\\n",
    "        + \"UNION {\" \\\n",
    "        + \"<\" + uri_subject + \"> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?uri . \" \\\n",
    "        + \"}\" \\\n",
    "        + \"UNION {\" \\\n",
    "        + \"<\" + uri_subject + \"> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?dt . \" \\\n",
    "        + \"?dt <http://www.w3.org/2002/07/owl#equivalentClass> ?uri \" \\\n",
    "        + \"}\" \\\n",
    "        + \"}\"\n",
    "        \n",
    "        \n",
    "    def createSPARQLQueryEquivalentClasses(self, uri_class):\n",
    "        return \"SELECT DISTINCT ?uri \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"{<\" + uri_class + \"> <http://www.w3.org/2002/07/owl#equivalentClass> ?uri .\" \\\n",
    "        + \"} \" \\\n",
    "        + \"UNION \" \\\n",
    "        + \"{ ?uri <http://www.w3.org/2002/07/owl#equivalentClass> <\" + uri_class + \"> .\" \\\n",
    "        + \"} \" \\\n",
    "        + \"}\";\n",
    "        \n",
    "        \n",
    "        \n",
    "    def createSPARQLQueryDistanceToAllSuperClassesForClass(self, uri_cls):\n",
    "        return \"SELECT  ?outA (count(?mid) as ?outB) \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"<\"+uri_cls+\"> <http://www.w3.org/2000/01/rdf-schema#subClassOf>* ?mid . \" \\\n",
    "        + \"?mid <http://www.w3.org/2000/01/rdf-schema#subClassOf>+ ?outA . \" \\\n",
    "        + \"}\" \\\n",
    "        + \"GROUP BY ?outA\";  \n",
    "        ##+ \"values ?uri_subject { <\" + uri_subject + \"> }\" \\\n",
    "        \n",
    "            \n",
    "        \n",
    "    def createSPARQLQueryAllSuperClassesForClass(self, uri_cls):\n",
    "            \n",
    "        return \"SELECT DISTINCT ?uri \" \\\n",
    "        + \"WHERE { \" \\\n",
    "        + \"{<\" + uri_cls + \"> <http://www.w3.org/2000/01/rdf-schema#subClassOf>* ?uri .\" \\\n",
    "        + \"}\" \\\n",
    "        + \"UNION {\" \\\n",
    "        + \"<\" + uri_cls + \"> <http://www.w3.org/2002/07/owl#equivalentClass> ?uri .\" \\\n",
    "        + \"}\" \\\n",
    "        + \"}\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def createSPARQLQueryAllSubClassesForClass(self, uri_cls):\n",
    "        \n",
    "        return \"SELECT DISTINCT ?uri \" \\\n",
    "        + \"WHERE { \" \\\n",
    "        + \"{?uri <http://www.w3.org/2000/01/rdf-schema#subClassOf>* <\" + uri_cls + \"> .\" \\\n",
    "        + \"}\" \\\n",
    "        + \"UNION {\" \\\n",
    "        + \"<\" + uri_cls + \"> <http://www.w3.org/2002/07/owl#equivalentClass> ?uri .\" \\\n",
    "        + \"}\" \\\n",
    "        + \"}\"\n",
    "    \n",
    "    \n",
    "    def createSPARQLQueryDistanceToAllSubClassesForClass(self, uri_cls):\n",
    "        \n",
    "\n",
    "        return \"SELECT  ?outA (count(?mid) as ?outB) \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"?mid <http://www.w3.org/2000/01/rdf-schema#subClassOf>* <\"+uri_cls+\"> . \" \\\n",
    "        + \"?outA <http://www.w3.org/2000/01/rdf-schema#subClassOf>+ ?mid . \" \\\n",
    "        + \"}\" \\\n",
    "        + \"GROUP BY ?outA\";  \n",
    "        ##+ \"values ?uri_subject { <\" + uri_subject + \"> }\" \\\n",
    "    \n",
    "    \n",
    "        \n",
    "    def createSPARQLQuerySameAsEntities(self, uri_entity):\n",
    "        return \"SELECT DISTINCT ?uri \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"{<\" + uri_entity + \"> <http://www.w3.org/2002/07/owl#sameAs> ?uri .\" \\\n",
    "        + \"} \" \\\n",
    "        + \"UNION {\" \\\n",
    "        + \"?uri <http://www.w3.org/2002/07/owl#sameAs> <\" + uri_entity + \"> .\" \\\n",
    "        + \"} \" \\\n",
    "        + \"}\";\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    ##TODO revise\n",
    "    def createSPARQLQuerySameAsEntities(self, uri_entity):\n",
    "        return \"SELECT DISTINCT ?uri \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"{<\" + uri_entity + \"> <http://www.wikidata.org/prop/direct/P460> ?uri .\" \\\n",
    "        + \"} \" \\\n",
    "        + \"UNION {\" \\\n",
    "        + \"?uri <http://www.wikidata.org/prop/direct/P460> <\" + uri_entity + \"> .\" \\\n",
    "        + \"} \" \\\n",
    "        + \"}\";\n",
    "        \n",
    "        \n",
    "        \n",
    "    def createSPARQLQueryAllSuperClassesForClass(self, uri_cls):\n",
    "        \n",
    "        return \"SELECT DISTINCT ?uri \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"<\" + uri_cls + \"> <http://www.wikidata.org/prop/direct/P279>+ ?uri \" \\\n",
    "        + \"}\";\n",
    "    \n",
    "    \n",
    "    def createSPARQLQueryDistanceToAllSuperClassesForClass(self, uri_cls):\n",
    "        \n",
    "\n",
    "        return \"SELECT  ?outA (count(?mid) as ?outB) \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"values ?uri_cls { <\" + uri_cls + \"> }\" \\\n",
    "        + \"?uri_cls <http://www.wikidata.org/prop/direct/P279>* ?mid .\" \\\n",
    "        + \"?mid <http://www.wikidata.org/prop/direct/P279>+ ?outA . \" \\\n",
    "        + \"}\" \\\n",
    "        + \"GROUP BY ?uri_cls ?outA\";  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def createSPARQLQueryAllSubClassesForClass(self, uri_cls):\n",
    "        \n",
    "        return \"SELECT DISTINCT ?uri \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"?uri <http://www.wikidata.org/prop/direct/P279>+ <\" + uri_cls + \">\" \\\n",
    "        + \"}\";\n",
    "    \n",
    "    \n",
    "    def createSPARQLQueryDistanceToAllSubClassesForClass(self, uri_cls):\n",
    "        \n",
    "\n",
    "        return \"SELECT  ?outA (count(?mid) as ?outB) \" \\\n",
    "        + \"WHERE {\" \\\n",
    "        + \"values ?uri_cls { <\" + uri_cls + \"> }\" \\\n",
    "        + \"?mid <http://www.wikidata.org/prop/direct/P279>* ?uri_cls .\" \\\n",
    "        + \"?outA <http://www.wikidata.org/prop/direct/P279>+ ?mid . \" \\\n",
    "        + \"}\" \\\n",
    "        + \"GROUP BY ?uri_cls ?outA\";  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    '''\n",
    "    TODO: Filter by schema.org, dbpedia or wikidata\n",
    "    '''\n",
    "\n",
    "    #ent=\"http://www.wikidata.org/entity/Q470813\" #Prim's algorithm\n",
    "    #ent=\"http://www.wikidata.org/entity/Q466575\" #middle-square method\n",
    "    #print(ent)\n",
    "    #ep = WikidataEndpoint()\n",
    "    #types = ep.getTypesForEntity(ent)\n",
    "    #print(len(types), types)\n",
    "    cls = \"http://dbpedia.org/ontology/Country\"\n",
    "    #cls = \"http://dbpedia.org/ontology/Person\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    ep = DBpediaEndpoint()\n",
    "    \n",
    "    # seconds passed since epoch\n",
    "    init = time.time()\n",
    "    \n",
    "    entities=set()\n",
    "    entities = ep.getEntitiesForType(cls, 0, 100)\n",
    "    print(\"Extracted entities: \", len(entities))\n",
    "    for ent in entities:\n",
    "        print(ent)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "\n",
    "    #local_time = time.ctime(seconds)\n",
    "    print(\"Time:\", end-init)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if True:\n",
    "        sys.exit(0) \n",
    "    \n",
    "\n",
    "\n",
    "    ent = \"http://dbpedia.org/resource/Scotland\"\n",
    "    #ent = \"http://dbpedia.org/resource/Allan_Pinkerton\"\n",
    "    #ent = 'http://www.wikidata.org/entity/Q22'\n",
    "    #\"http://dbpedia.org/resource/Hern%C3%A1n_Andrade\"\n",
    "    ent=\"http://dbpedia.org/resource/Chicago_Bulls\"\n",
    "    ep = DBpediaEndpoint()\n",
    "    types = ep.getTypesForEntity(ent)\n",
    "    print(len(types), types)\n",
    "    \n",
    "    \n",
    "    sameas = ep.getSameEntities(ent)\n",
    "    print(len(sameas), sameas)\n",
    "    \n",
    "    \n",
    "    \n",
    "    labels = ep.getEnglishLabelsForEntity(ent)\n",
    "    print(len(labels), labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cls = \"http://dbpedia.org/ontology/BaseballTeam\"\n",
    "    \n",
    "    print(\"Domain types\")\n",
    "    types_domain = ep.getTopTypesUsingPredicatesForSubject(ent, 3)\n",
    "    for t in types_domain:\n",
    "        print(t)\n",
    "    \n",
    "    print(\"Range types\")\n",
    "    types_range = ep.getTopTypesUsingPredicatesForObject(ent, 3)\n",
    "    for t in types_range:\n",
    "        print(t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cls = \"http://dbpedia.org/ontology/Country\"\n",
    "    #cls = \"http://dbpedia.org/ontology/Person\"\n",
    "    #cls = 'http://www.wikidata.org/entity/Q6256'\n",
    "    \n",
    "   \n",
    "    sup2dist = ep.getDistanceToAllSuperClasses(cls)\n",
    "    print(len(sup2dist), sup2dist)\n",
    "    \n",
    "    sub2dist = ep.getDistanceToAllSubClasses(cls)\n",
    "    print(len(sub2dist), sub2dist)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ep = WikidataEndpoint()\n",
    "    #types = ep.getAllTypesForEntity(\"http://www.wikidata.org/entity/Q22\")\n",
    "    #print(len(types), types)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #equiv = ep.getEquivalentClasses(cls)\n",
    "    #print(len(equiv), equiv)\n",
    "    \n",
    "    \n",
    "    #same = ep.getSameEntities(ent)\n",
    "    #print(len(same), same)\n",
    "    \n",
    "    \n",
    "    gt_cls=\"http://www.wikidata.org/entity/Q5\"\n",
    "    \n",
    "    sup2dist = ep.getDistanceToAllSuperClasses(gt_cls)\n",
    "    print(len(sup2dist), sup2dist)\n",
    "    \n",
    "    \n",
    "    sub2dist = ep.getDistanceToAllSubClasses(gt_cls, 2)\n",
    "    print(len(sub2dist), sub2dist)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ent=\"http://www.wikidata.org/entity/Q22\"\n",
    "    ent=\"http://www.wikidata.org/entity/Q5\"\n",
    "    #ent=\"http://www.wikidata.org/entity/Q128109\"\n",
    "    labels = ep.getEnglishLabelsForEntity(ent)\n",
    "    print(len(labels), labels)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"C:\\\\Users\\\\01-18-20\\\\Documents\\\\City_University\\\\round3\\\\CEA_round3_DB_get.csv\")\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"col_Entity\"]=''\n",
    "for i, row in df2.iterrows():\n",
    "    #print(i)\n",
    "    ent1 = df2.loc[i][\"Wikidata_Entity\"]\n",
    "    print(ent1)\n",
    "    ep = DBpediaEndpoint()\n",
    "    sameas = ep.getSameEntities(ent1) # to get the same entity from the wikidata by using DBpedia entity\n",
    "    df2[\"col_Entity\"][i]=sameas\n",
    "    print(sameas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.to_csv('Round3_cea_missing_DB_WIKI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code is used for getting class for the column in CTA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CEA result file\n",
    "df1 = pd.read_csv(\"C:\\\\Users\\\\01-18-20\\\\Documents\\\\City_University\\\\Project_Masters_Thesis_CTA_CPA_CEA\\\\CTA_TASK\\\\CEA_END_RESULT_v03.csv\")\n",
    "rslt_df = df1[df1['Column_id'] == 0]\n",
    "rslt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df[\"col_Entity\"]='' # add the column in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function WikidataEndpoint \n",
    "for i, row in rslt_df.iterrows():\n",
    "    #print(i)\n",
    "    ent1 = rslt_df.loc[i][\"Wikidata_Entity\"]\n",
    "    print(ent1)\n",
    "    ep = WikidataEndpoint()\n",
    "    types = ep.getTypesForEntity(ent1)\n",
    "    rslt_df[\"col_Entity\"][i]=types\n",
    "    print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rslt_df.to_csv('final.csv') # for getting the output for CTA task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
